{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import typing\n",
    "from typing import List, Dict \n",
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4503b35",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file):\n",
    "    with open(file, \"r\", encoding=\"utf-8\") as f:\n",
    "        data = json.load(f)\n",
    "    return (data)\n",
    "\n",
    "def save_data(file, data):\n",
    "    with open (file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63018f30",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\aubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77cdde2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a random sample from the dataset\n",
    "\n",
    "files = reuters.fileids()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "969602f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EQUION CORP &lt;EQUI> 2ND QTR JAN 31 NET\n",
      "  Oper shr 19 cts vs 18 cts\n",
      "      Oper net 951,902 vs 987,860\n",
      "      Revs 19.0 mln vs 17.1 mln\n",
      "      Six mths\n",
      "      Oper shr 26 cts vs 35 cts\n",
      "      Oper net 1,332,273 vs 2,502,868\n",
      "      Revs 33.6 mln vs 29.2 mln\n",
      "      Note: Oper net excludes tax credits of 897,925 dlrs vs\n",
      "  841,511,dlrs for qtr and 1,306,860 dlrs vs 2,132,073 dlrs for\n",
      "  six mths.\n",
      "      Note: Year-ago results restated to reflect change in\n",
      "  accounting principle effective August one, 1985.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(files, 1)\n",
    "\n",
    "print(reuters.raw(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bac81bbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WHIRLPOOL CORP &lt;WHR> 1ST QTR NET\n",
      "  Shr 66 cts vs 67 cts\n",
      "      Net 48,700,000 vs 49,300,000\n",
      "      Sales 961.0 mln vs 870.6 mln\n",
      "      Avg shrs 74,123,837 vs 73,374,398\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(files, 1)\n",
    "\n",
    "print(reuters.raw(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c629fb92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.S. BUSINESS LOANS FELL 822 MLN DLRS IN MARCH 25 WEEK, FED SAYS\n",
      "\n",
      "  U.S. BUSINESS LOANS FELL 822 MLN DLRS IN MARCH 25 WEEK, FED SAYS\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(files, 1)\n",
    "\n",
    "print(reuters.raw(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "01576fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "U.K. MONEY MARKET DEFICIT FORECAST AT 450 MLN STG\n",
      "  The Bank of England said it forecast a\n",
      "  shortage of around 450 mln stg in the money market today.\n",
      "      Among the main factors affecting liquidity, bills maturing\n",
      "  in official hands and the take-up of treasury bills will drain\n",
      "  some 650 mln stg while a rise in note circulation will take out\n",
      "  around 30 mln stg.\n",
      "      Partly offsetting these outflows, bankers' balances above\n",
      "  target and exchequer transactions will add some 200 mln stg and\n",
      "  35 mln stg to the system respectively.\n",
      "  \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(files, 1)\n",
    "\n",
    "print(reuters.raw(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "151c8376",
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 4 main types of articles. The first 3 mentioned above do not hold any information about relation but simply about revenues, both quarterly and yearly.\n",
    "# For a prelimnary analysis we will be ignoring these. One easy way of ignoring these is by excluding all articles that contain the word \"qtr\", the other one is where \n",
    "# \"vs\" is mentioned at least twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0444913",
   "metadata": {},
   "outputs": [],
   "source": [
    "suitable_articles = []\n",
    "suitable_ids = []\n",
    "\n",
    "for article_id in files:\n",
    "    if reuters.raw(article_id).count(\"vs\") <= 1:\n",
    "        suitable_articles.append(reuters.raw(article_id))\n",
    "        suitable_ids.append(article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "85efd212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['YEUTTER SAYS U.S. BUDGET DEFICIT REDUCTION KEY TO TRADE DEFICIT SOLUTION\\n\\n  YEUTTER SAYS U.S. BUDGET DEFICIT REDUCTION KEY TO TRADE DEFICIT SOLUTION\\n  \\n\\n']\n"
     ]
    }
   ],
   "source": [
    "sample = random.sample(suitable_articles, 1)\n",
    "\n",
    "print(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fd5373f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8399"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This gives us a list of 9134 articles that are suitable to be analyzed for relations between entities.\n",
    "\n",
    "len(suitable_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3696b400",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')\n",
    "txt = suitable_articles[0]\n",
    "doc = nlp(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c4b7478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['tok2vec', 'tagger', 'parser', 'attribute_ruler', 'lemmatizer', 'ner']\n"
     ]
    }
   ],
   "source": [
    "print(nlp.pipe_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "36e47ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_proper_nouns(doc) -> List[str]:\n",
    "    # grab list of indexes proper noun positive matches\n",
    "    pos = [tok.i for tok in doc if tok.pos_ == \"PROPN\"]\n",
    "    consecutives = []\n",
    "    current = []\n",
    "\n",
    "    # loop over proper noun position and write to the data\n",
    "    for elt in pos:\n",
    "        if len(current) == 0:\n",
    "            current.append(elt)\n",
    "        else:\n",
    "            if current[-1] == elt - 1:\n",
    "                current.append(elt)\n",
    "            else:\n",
    "                consecutives.append(current)\n",
    "                current = [elt]\n",
    "    if len(current) != 0:\n",
    "        consecutives.append(current)\n",
    "    return [doc[consecutive[0]:consecutive[-1]+1] for consecutive in consecutives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ff775cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning functionality\n",
    "def clean_sentence(sentence):\n",
    "    lemmas=[]\n",
    "    for word in sentence:\n",
    "        lemmas.append(word.lemma_)\n",
    "    lemma_string = ' '.join(lemmas)\n",
    "    txt_no_return = lemma_string.replace(\"\\n\", \" \") # remove \"/n\"\n",
    "    text_without_punct = txt_no_return.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    text_one_space = re.sub(' +', ' ', text_without_punct) # remove multiple spaces\n",
    "    text_clean = text_one_space.strip()\n",
    "    return text_clean\n",
    "\n",
    "def get_doc(article):\n",
    "    doc = nlp(article)\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        sentences.append(clean_sentence(sent))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a71aabc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10788/10788 [19:54<00:00,  9.03it/s] \n"
     ]
    }
   ],
   "source": [
    "# There are 4 main types of articles. The first 3 mentioned above do not hold any information about relation but simply about revenues, both quarterly and yearly.\n",
    "# For a prelimnary analysis we will be ignoring these. One easy way of ignoring these is by excluding all articles that contain the word \"qtr\", the other one is where \n",
    "# \"vs\" is mentioned at least twice.\n",
    "\n",
    "suitable_articles = {}                                   \n",
    "                                                          \n",
    "for article_id in tqdm.tqdm(files):                                  \n",
    "    if reuters.raw(article_id).count(\"vs\") <= 1:   #This gives us a list of 9134 articles that are suitable to be analyzed for relations between entities.       \n",
    "        suitable_articles[article_id] = get_doc(reuters.raw(article_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "47e36b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"docs.obj\",\"wb\")\n",
    "pickle.dump(suitable_articles, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "90d3afd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"docs.obj\",'rb')\n",
    "cleaned_articles = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85de3d15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8399/8399 [06:40<00:00, 20.95it/s]\n"
     ]
    }
   ],
   "source": [
    "nouns_per_article = {}\n",
    "\n",
    "for key, cleaned_text in tqdm.tqdm(suitable_articles.items()):\n",
    "    text = \"\"\n",
    "    for sentence in cleaned_text:\n",
    "        text += \" \" + sentence\n",
    "    doc = nlp(text)\n",
    "    nouns_per_article[key] = extract_proper_nouns(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ab6497cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nouns_str_per_article = {}\n",
    "\n",
    "for key,val in nouns_per_article.items():\n",
    "    nouns_str_per_article[key] = [v.text for v in val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c39286b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"nouns.obj\",\"wb\")\n",
    "pickle.dump(nouns_str_per_article, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e441d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open(\"nouns.obj\",'rb')\n",
    "nouns_str_per_article = pickle.load(file)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "330c68c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prelimanary count of all the proper nouns I can find. I couldn't track a working spacy method so I converted to string and used native Counter.\n",
    "# Also made 2 lists, one containing all unique values and 1 containing all values, both as strings.\n",
    "\n",
    "all_entities = []\n",
    "counted_entities = Counter()\n",
    "unique_entities = set()\n",
    "\n",
    "for key, val in nouns_str_per_article.items():\n",
    "    counted_entities += Counter(val)\n",
    "    all_entities.append(val)\n",
    "\n",
    "all_entities = [item for sublist in all_entities for item in sublist]\n",
    "unique_entities = set(all_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cd5f06ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the ones with a minimum of 3 links\n",
    "\n",
    "counted_entities_thresh = Counter({k: c for k, c in counted_entities.items() if c >= 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74630e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom difflib import SequenceMatcher\\n\\nsimilarity = []\\nunique_list = list(unique_entities)\\n\\n#for val_1 in counted_entities.most_common()[0]:\\nfor val_2 in tqdm.tqdm(unique_list):\\n    similarity.append(SequenceMatcher(None, str(counted_entities.most_common()[0][0]), str(val_2)).ratio())\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Following method is a slow way of calculating the similarities between the elements.\n",
    "\"\"\"\n",
    "from difflib import SequenceMatcher\n",
    "\n",
    "similarity = []\n",
    "unique_list = list(unique_entities)\n",
    "\n",
    "#for val_1 in counted_entities.most_common()[0]:\n",
    "for val_2 in tqdm.tqdm(unique_list):\n",
    "    similarity.append(SequenceMatcher(None, str(counted_entities.most_common()[0][0]), str(val_2)).ratio())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "437e7285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.99989045897688694655\r"
     ]
    }
   ],
   "source": [
    "\n",
    "from Levenshtein import ratio\n",
    "\n",
    "lev_similarity = x = [[] for i in range(len(counted_entities_thresh))]\n",
    "unique_list = [val for val in counted_entities_thresh]\n",
    "\n",
    "for idx, val_1 in enumerate(counted_entities_thresh):\n",
    "    print(idx/len(counted_entities_thresh),end=\"\\r\")\n",
    "    for val_2 in counted_entities_thresh:\n",
    "        lev_similarity[idx].append(ratio(val_1, val_2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "587152d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "monday\n",
      "0.5\n",
      "Beaumont Texas\n",
      "0.5\n",
      "Calmon\n",
      "0.5\n",
      "La Paz\n",
      "0.5\n",
      "National Pizza\n",
      "0.5\n",
      "Ramon del Rosario\n",
      "0.5185185185185185\n",
      "Raymond Stone\n",
      "0.5217391304347826\n",
      "Rha Woong Bae\n",
      "0.5217391304347826\n",
      "Richmond Hill\n",
      "0.5217391304347826\n",
      "San Diego\n",
      "0.5263157894736842\n",
      "van Driel\n",
      "0.5263157894736842\n",
      "Van Driel\n",
      "0.5263157894736842\n",
      "Ron Paice\n",
      "0.5263157894736842\n",
      "Tasmania\n",
      "0.5555555555555556\n",
      "Diaz\n",
      "0.5714285714285714\n",
      "Romania\n",
      "0.5882352941176471\n",
      "Ramirez\n",
      "0.5882352941176471\n",
      "Raymond\n",
      "0.5882352941176471\n",
      "Raimond\n",
      "0.5882352941176471\n",
      "Ramon Diaz\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "y = 267\n",
    "\n",
    "for idx in sorted(range(len(lev_similarity[y])), key=lambda x: lev_similarity[y][x])[-20:]:\n",
    "    print(unique_list[idx])\n",
    "    print(lev_similarity[y][idx])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "59ffe63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_links(search_term, threshold_links):\n",
    "    counted = Counter()\n",
    "\n",
    "    for _, ent in nouns_str_per_article.items():\n",
    "        if search_term in [x for x in ent]:\n",
    "            counted += Counter([x for x in ent])\n",
    "\n",
    "    counted = Counter({k: c for k, c in counted.items() if c >= threshold_links})\n",
    "\n",
    "    return counted.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "32fbfe86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_links(links, name):\n",
    "    df = pd.DataFrame(links)\n",
    "    df.to_csv(name)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32f3917c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Multifoods', 5), ('Bregman Partners', 4)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_links('Multifoods', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "07a66107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Harcourt', 44),\n",
       " ('Harper', 23),\n",
       " ('Row', 9),\n",
       " ('Robert Maxwell', 6),\n",
       " ('Harcourt Brace Jovanovich Inc', 6),\n",
       " ('Maxwell', 6),\n",
       " ('BPCC', 6),\n",
       " ('Reed', 6),\n",
       " ('HARCOURT', 5),\n",
       " ('June', 5),\n",
       " ('Harcourt Brace', 4),\n",
       " ('New York', 3),\n",
       " ('Printing', 3),\n",
       " ('stg', 3),\n",
       " ('HARPER', 3),\n",
       " ('March', 3)]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_links('Harcourt', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b3fc6686",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Crazy Eddie', 40),\n",
       " ('Entertainment Marketing', 15),\n",
       " ('Crazy Eddie Inc', 10),\n",
       " ('CRAZY EDDIE', 5),\n",
       " ('Antar', 5),\n",
       " ('June', 5),\n",
       " ('April', 4),\n",
       " ('Zinn', 4),\n",
       " ('lt;CRZY', 3),\n",
       " ('Eddie', 3),\n",
       " ('Entertainment Marketing Inc', 3),\n",
       " ('lt;EM', 3),\n",
       " ('Entertainment', 3),\n",
       " ('Belzberg', 3),\n",
       " ('May', 3),\n",
       " ('dlrs', 3),\n",
       " ('SEC', 3)]"
      ]
     },
     "execution_count": 266,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_links('Crazy Eddie', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "21925fae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Delta', 18),\n",
       " ('Western', 7),\n",
       " (\"O'Connor\", 4),\n",
       " ('DELTA', 3),\n",
       " ('lt;DAL', 3),\n",
       " ('lt;WAL', 3),\n",
       " ('Appeals', 3),\n",
       " ('Supreme Court', 3),\n",
       " ('PNB', 3),\n",
       " ('Toyota', 3),\n",
       " ('April', 3)]"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_links('Delta', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "047dd85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Japan', 52),\n",
       " ('Sumita', 25),\n",
       " ('Paris', 24),\n",
       " ('Bank', 24),\n",
       " ('Satoshi Sumita', 16),\n",
       " ('U.S.', 13),\n",
       " ('April', 11),\n",
       " ('SUMITA', 10),\n",
       " ('Canada', 7),\n",
       " ('Britain', 7),\n",
       " ('Group', 6),\n",
       " ('Dealers', 6),\n",
       " ('France', 6),\n",
       " ('Bundesbank', 5),\n",
       " ('February', 5),\n",
       " ('JAPAN', 5),\n",
       " ('Baker', 4),\n",
       " ('Washington', 4),\n",
       " ('Lower House Budget Committee', 4),\n",
       " ('Parliament', 4),\n",
       " ('Plaza', 4),\n",
       " ('Tokyo', 4),\n",
       " ('U.S. Treasury Secretary James Baker', 4),\n",
       " ('West Germany', 4)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_links('Satoshi Sumita', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0e719e13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = save_links(find_links('Satoshi Sumita', 4), 'Satoshi.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97988e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"linked\"] = [\"Satoshi Sumita\" for x in range(len(df))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3ee13475",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvis.network import Network\n",
    "import pandas as pd\n",
    "\n",
    "got_net = Network(height='100%', width='100%', bgcolor='#222222', font_color='white')\n",
    "\n",
    "# set the physics layout of the network\n",
    "got_net.barnes_hut()\n",
    "got_data = df\n",
    "\n",
    "sources = got_data['linked']\n",
    "targets = got_data[0]\n",
    "weights = got_data[1] * 50\n",
    "\n",
    "edge_data = zip(sources, targets, weights)\n",
    "\n",
    "for e in edge_data:\n",
    "    src = e[0]\n",
    "    dst = e[1]\n",
    "    w = e[2]\n",
    "\n",
    "    got_net.add_node(src, src, title=src)\n",
    "    got_net.add_node(dst, dst, title=dst)\n",
    "    got_net.add_edge(src, dst, value=w)\n",
    "\n",
    "neighbor_map = got_net.get_adj_list()\n",
    "\n",
    "# add neighbor data to node hover data\n",
    "for node in got_net.nodes:\n",
    "    node['title'] += ' Neighbors:<br>' + '<br>'.join(neighbor_map[node['id']])\n",
    "    node['value'] = len(neighbor_map[node['id']])\n",
    "\n",
    "got_net.show('links.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f26a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273019ac",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'unique_entities' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_20800/3708819914.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mlev_similarity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0munique_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0munique_entities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mlev_similarity\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mratio\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Crazy Eddie Inc\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"CRAZY EDDY\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \"\"\"\n",
      "\u001b[1;31mNameError\u001b[0m: name 'unique_entities' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "from Levenshtein import ratio\n",
    "\n",
    "lev_similarity = []\n",
    "unique_list = list(unique_entities)\n",
    "lev_similarity.append(ratio(\"Crazy Eddie Inc\", \"CRAZY EDDY\"))\n",
    "\"\"\"\n",
    "#for val_1 in counted_entities.most_common()[0]:\n",
    "for val_2 in tqdm.tqdm(unique_list):\n",
    "    lev_similarity.append(ratio(str(counted_entities_thresh.most_common()[0][0]), str(val_2)))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb8eed13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78fb004",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1b6ecfbee8e8b49531eec9b524a3d6fbe86e8ef726d9507ad5a8d168d7bc61e3"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('NLP': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
