{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import random\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "import typing\n",
    "from typing import List, Dict \n",
    "import tqdm\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import string \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\aubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\aubin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the dataset\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import reuters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# There are 4 main types of articles. The first 3 mentioned above do not hold any information about relation but simply about revenues, both quarterly and yearly.\n",
    "# For a prelimnary analysis we will be ignoring these. One easy way of ignoring these is by excluding all articles that contain the word \"qtr\", the other one is where \n",
    "# \"vs\" is mentioned at least twice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "company_tickers = []\n",
    "\n",
    "files = reuters.fileids()\n",
    "\n",
    "suitable_articles = []\n",
    "suitable_ids = []\n",
    "\n",
    "for article_id in files:\n",
    "    if reuters.raw(article_id).count(\"vs\") <= 1:\n",
    "        suitable_articles.append(reuters.raw(article_id))\n",
    "        suitable_ids.append(article_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data cleaning functionality\n",
    "def clean_articles(sentence):\n",
    "\n",
    "    tbc_sentence = sentence\n",
    "\n",
    "    for i in range(tbc_sentence.count(\"&lt\")):\n",
    "        try:\n",
    "            start = tbc_sentence.index(\"&lt\")\n",
    "            end = tbc_sentence.index(\">\")\n",
    "\n",
    "            result = tbc_sentence[start + 4:end]\n",
    "            company_tickers.append(result)\n",
    "\n",
    "            tbc_sentence = tbc_sentence.replace(tbc_sentence[start:end+1], '')\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    txt_no_return = tbc_sentence.replace(\"\\n\", \" \") # remove \"/n\"\n",
    "    txt_lowered = txt_no_return.lower()\n",
    "    txt_no_lt = txt_lowered.replace('&lt', ' ')\n",
    "    #text_without_punct = txt_no_lt.translate(str.maketrans('', '', string.punctuation)) # remove punctuation\n",
    "    text_one_space = re.sub(' +', ' ', txt_no_lt) # remove multiple spaces\n",
    "    text_clean = text_one_space.strip()\n",
    "    return text_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles_cleaned = {}\n",
    "\n",
    "for idx, article in enumerate(suitable_articles):\n",
    "    articles_cleaned[suitable_ids[idx]] = clean_articles(article)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"../stored_data/articles_cleaned.obj\",\"wb\")\n",
    "pickle.dump(articles_cleaned, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "filehandler = open(\"../stored_data/company_tickers.obj\",\"wb\")\n",
    "pickle.dump(company_tickers, filehandler)\n",
    "filehandler.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d77edf4d22c95116f7db69775c20b4e4dae950b1cce7917aaff0b99c27fca43a"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 64-bit ('nlp-new': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
